# -*- coding: utf-8 -*-"""Author(s):                Roshan Patel <roshanp@princeton.edu>Contributor(s):                Michael Webb <mawebb@princeton.edu>"""import argparseimport time,osimport pickle as pklimport numpy as npimport sysfrom functools import partialfrom joblib import Parallel,delayedimport tensorflow as tffrom sklearn.model_selection    import KFold,StratifiedKFoldfrom sklearn.metrics            import r2_score, mean_absolute_error,mean_squared_errorfrom hyperopt import fmin,tpe,hp,space_eval,trials_from_docs,Trialsfrom spektral.data import DisjointLoaderfrom spektral.layers import GCNConv,GATConv,GlobalAvgPool,GlobalSumPool,GlobalMaxPoolfrom polyfeat import polymer_graphfrom hyperopt.pyll.base import scopefrom training_utils import write_params,trial_loader, create_dir, splitIDs, EarlyStoppingAtMinLossdef create_parser():    parser = argparse.ArgumentParser(description='Implementation for training graph neural network.')    parser.add_argument('-graph_dir'   ,default=None,help = 'Name of the directory that has all of the graphs. Labels are a part of the graph structure (default = None')    parser.add_argument('-job'  ,default=None,help = 'Enter the number of the fold that this script will be handling')    parser.add_argument('-outerk'  ,default=None,help = 'Enter the number of the fold that this script will be handling')    parser.add_argument('-foldnumber'  ,default=None,help = 'Enter the number of the fold that this script will be handling')    parser.add_argument('-innerk'  ,default=None,help = 'Enter the number of the fold that this script will be handling')    parser.add_argument('-dataID',default=None,help='These ids are used to keep track of data varients produced via data augmentation')    return parserdef bs(data,batch_size):    if batch_size >= len(data):        return batch_size    else:        return len(data)def partition(indicies,n=1000):    indicies = np.array(indicies)    if n >= len(indicies):        return [indicies]    else:        packets = []        remaining_indices = len(indicies)        i = 0        while remaining_indices >= n:            subset = indicies[i*n:(i+1)*n]            packets.append(subset)            i += 1            remaining_indices -= n        subset = indicies[-remaining_indices:]        packets.append(subset)    return packetsdef custom_training_loop(train_index,val_index,params,graph_dir,maxloadgraph):    patience = 50    reduce_on_p = 15    best_loss= np.Inf    best_weights = None    wait = 0    model = GNNmodel(params)    batch_size = params['bs']    lr = 0.01    loss_fn = tf.keras.losses.MeanSquaredError()    print('Beginning Training for {} Epochs'.format(350))    for i in range(1,351):        print('Epoch {}/{}'.format(i,350))        #training        training_loss = 0        t_c = 0        train_splits = partition(train_index,n=maxloadgraph)        for train_ids in train_splits:            t_c += len(train_ids)            train_data = polymer_graph(dirname = graph_dir,ids = train_ids)            train_bs = bs(train_ids,batch_size)            train_loader = DisjointLoader(train_data,batch_size=train_bs,epochs=1,shuffle=True)            optimizer = tf.keras.optimizers.Adam(learning_rate = lr)            for i,(inputs,labels) in enumerate(train_loader):                with tf.GradientTape() as tape:                    tape.watch(model.trainable_variables)                    y_predict = model(inputs,training = True)                    loss = loss_fn(labels,y_predict)                variables = tape.watched_variables()                gradients = tape.gradient(loss,variables)                optimizer.apply_gradients(zip(gradients,variables))                training_loss += loss        val_loss,v_c = evaluate_model(val_index, model, params,graph_dir,maxloadgraph)        print('Training Loss / graph = {:.2f}'.format(training_loss / t_c))        norm_val = val_loss / v_c        print('Val Loss / graph = {:.2f}'.format(norm_val))        #adjust learning rate / deploy early stopping routine        if norm_val < best_loss:            wait = 0            best_weights = model.get_weights()            best_loss = norm_val        else:            wait+=1        if wait == patience:            model.set_weights(best_weights)            print('Restoring weights...')            print('Ending training on epoch {}'.format(i))        if wait == reduce_on_p:            lr*=0.1    return modeldef evaluate_model(test_idx,trained_model,params,graph_dir,maxloadgraph,final_eval=False,summariespath=None,foldnumber=None):    batch_size = params['bs']    test_loss = 0    loss_fn = tf.keras.losses.MeanSquaredError()    tc = 0    testing_splits = partition(test_idx,maxloadgraph)    all_y_predict = []    all_labels = []    for test_ids in testing_splits:        tc+= len(test_ids)        test_data = polymer_graph(dirname = graph_dir,ids = test_ids)        test_bs = bs(test_ids,batch_size)        test_loader = DisjointLoader(test_data,batch_size=test_bs,epochs=1,shuffle=True)        for i,(inputs,labels) in enumerate(test_loader):            y_predict = trained_model(inputs,training=False)            yp = np.array(y_predict,dtype='float').flatten()            lab = np.array(labels,dtype ='float').flatten()            all_y_predict.append(yp)            all_labels.append(lab)            loss = loss_fn(labels,y_predict)            test_loss += loss    if final_eval:        all_y_predict = np.array(all_y_predict).flatten()        all_labels = np.array(all_labels).flatten()        r2 = r2_score(all_labels,all_y_predict)        mae = mean_absolute_error(all_labels,all_y_predict)        with open('{}/fold{}_predictions.txt'.format(summariespath,foldnumber),'a') as fid:            for i in range(len(all_y_predict)):                fid.write(str(all_y_predict[i]) + ' ' + str(all_labels[i]))                fid.write('\n')        return r2,mae    return test_loss,tcclass GNNmodel(tf.keras.Model):    def __init__(self,params):        super().__init__()        if params['type'] == 'GCN':            self.layer1 = GCNConv(params['output_dim'],activation = params['af'])        elif params['type'] == 'GAT':            self.layer1 = GATConv(params['output_dim'],activation = params['af'])        else:            sys.exit('Invalid layer 1')        if params['graph_layer2']['state'] == 'present':            if params['graph_layer2']['type'] == 'GCN':                self.layer2 = GCNConv(params['graph_layer2']['output_dim'],activation = params['graph_layer2']['af'])            elif params['graph_layer2']['type'] == 'GAT':                self.layer2 = GATConv(params['graph_layer2']['output_dim'],activation = params['graph_layer2']['af'])            else:                sys.exit('Invalid layer 2')        else:            self.layer2 = None        #create the pooling layer        if params['pooling'] == 'sum':            self.pool = GlobalSumPool()        elif params['pooling'] == 'avg':            self.pool = GlobalAvgPool()        elif params['pooling'] == 'max':            self.pool = GlobalMaxPool()        else:            sys.exit('Invalid pooling layer')        #create the final neural network layers        self.dense1 = tf.keras.layers.Dense(params['s1'],activation='relu')        self.drop1 = tf.keras.layers.Dropout(params['d1'])        if params['layer2']['state'] == 'present':            self.dense2 = tf.keras.layers.Dense(params['layer2']['s2'],activation = 'relu')            self.drop2  = tf.keras.layers.Dropout(params['layer2']['d2'])        elif params['layer2']['state'] =='Absent':            self.dense2 = None            self.drop2  = None        self.final = tf.keras.layers.Dense(1)    def call(self,inputs):        x,a,i = inputs        y = self.layer1([x, a])        if self.layer2 is not None: y = self.layer2([y, a])        y = self.pool([y, i])        y = self.dense1(y)        y = self.drop1(y)        if self.dense2 is not None: y = self.dense2(y)        if self.drop2 is not None: y = self.drop2(y)        y = self.final(y)        return ydef return_paramspace():    paramSpace = {        'type':hp.choice('layer1_type',options = ['GCN','GAT']),        'output_dim':scope.int(hp.quniform('output_dim1', 2, 42, 4)),        'af':hp.choice('af1',['relu',None]),        'graph_layer2':hp.choice('graph_layer2',options = [            {            'state':'present',            'type':hp.choice('layer2_choice',options = ['GCN','GAT']),            'output_dim':scope.int(hp.quniform('output_dim3', 2, 42, 4)),            'af':hp.choice('af3',['relu',None])            },            {            'state':'absent'            }            ]),        'pooling':hp.choice('pooling',['sum','avg']),        #layer 1 options        's1':scope.int(hp.quniform('s1', 10, 750, 20)),        'd1':hp.quniform('d1', 0, 0.8, 0.1),        #layer 2 options        'layer2':hp.choice('layer2',[            {            'state':'present',            's2':scope.int(hp.quniform('s2', 10, 750, 20)),            'd2':hp.quniform('d2', 0, 0.8, 0.1)            },            {            'state':'Absent'            },            ]),        'bs':hp.choice('bs',[32,64,128,256])        }    return paramSpacedef objective_function(params,graph_dir,ids,trainID,summariespath,foldnumber,innerfold,trials,maxloadgraph):    temp_path = '{}/fold{}_hpsummary.txt'.format(summariespath,foldnumber)    with open(temp_path,'a') as fid:        fid.write('~~~~~~Trial {} Hyperparameters~~~~~~~'.format(len(trials)))        fid.write('\n')    write_params(params,path=temp_path)    with open('{}/fold{}_trials.pkl'.format(summariespath,foldnumber),'wb') as fid:        pkl.dump(trials,fid)    kfold2 = KFold(n_splits=innerfold,shuffle=True,random_state=27)    splits2 = [(trainID[train],trainID[test]) for (train,test) in kfold2.split(trainID)]    mses = Parallel(n_jobs=innerfold)(delayed(parallel_k2)(split,params,graph_dir,ids,maxloadgraph) for split in splits2)    loss = np.mean(mses)    with open(temp_path,'a') as fid:        fid.write('Loss: {:.2f}'.format(loss) + '\n')    return lossdef parallel_k2(split,params,graph_dir,ids,maxloadgraph):    (i_trainID, i_testID) = split    #take 15 percent of the training data for validation a    np.random.shuffle(i_trainID)    val_split = 0.15    split_index = int(len(i_trainID) * val_split)    train_ID= i_trainID[split_index:]    val_ID = i_trainID[:split_index]    i_train_index,i_test_index = splitIDs(train_ID,i_testID,ids)    i_train_index,i_val_index  = splitIDs(train_ID,val_ID,ids)    #train the model and get the loss associated with these parameters    trained_model = custom_training_loop(i_train_index, i_val_index, params,graph_dir,maxloadgraph)    test_loss = evaluate_model(i_test_index, trained_model, params,graph_dir,maxloadgraph)    return test_lossdef main():    parser = create_parser()    args   = parser.parse_args()    graph_dir  = args.graph_dir    idpath    = args.dataID    resultspath = '{}/results'.format(args.job)    summariespath = '{}/summaries'.format(args.job)    create_dir(args.job)    create_dir(resultspath)    create_dir(summariespath)    outerfold = int(args.outerk)    foldnumber= int(args.foldnumber)    innerfold = int(args.innerk)    maxloadgraph = 2000    #create the dir that has a summary of the outputs    if foldnumber == 0:        create_dir(summariespath)    graph1 = polymer_graph(dirname=graph_dir,ids=np.array([1]))    #load in the dataids. Make sure nothing else is in the graph_dir!    if idpath is not None:        with open(idpath,'rb') as fid:            ids = pkl.load(fid)    else:        ids = np.arange(len(os.listdir(graph_dir)))    unique_ids = np.unique(ids)    #this is to approximately stratify by the DP    counter = 0    tags = []    for i in range(int(len(unique_ids)/5)):        for j in range(5):            tags.append(counter)        counter+=1    tags = np.array(tags)    #run k=5 fold to evaluate the model on different splits of the data    kfold1 = StratifiedKFold(n_splits = 5,shuffle=True,random_state = 27)    trainID = [unique_ids[train] for (train,test) in kfold1.split(X=unique_ids,y=tags)][foldnumber]    testID  = [unique_ids[test] for (train,test) in kfold1.split(X=unique_ids,y=tags)][foldnumber]    #determine the best hyperparameters    try:        trials = trial_loader('trials',summariespath,resultspath)    except:        trials = Trials()    paramSpace = return_paramspace()    fmin_objective = partial(objective_function,                             graph_dir = graph_dir,                             ids = ids,                             trainID = trainID,                             summariespath = summariespath,                             foldnumber = foldnumber,                             innerfold = innerfold,                             trials = trials,                             maxloadgraph = maxloadgraph                             )    best = fmin(          fn=fmin_objective,          space=paramSpace,          algo=tpe.suggest,          max_evals=200,          trials=trials)    best_params = space_eval(paramSpace, best)    split_id = int(len(trainID) * 0.15)    val = trainID[:split_id]    train = trainID[split_id:]    train_index,test_index = splitIDs(train, testID, ids)    train_index,val_index = splitIDs(train,val,ids)    #build the model, write out the best parameters and the performance    batch_size = best_params['bs']    trained_model = custom_training_loop(train_index, val_index, best_params, graph_dir, maxloadgraph)    r2,mae = evaluate_model(test_index, trained_model, best_params,graph_dir,maxloadgraph,final_eval=True,summariespath=summariespath,foldnumber=foldnumber)    #write out the results    temp_path = '{}/Model_{}_Hyperparameters.txt'.format(resultspath,foldnumber)    with open(temp_path,'a') as fid:        fid.write('~~~~~~Fold {} Hyperparameters~~~~~~~'.format(foldnumber))        fid.write('\n')    write_params(best_params,path=temp_path)    with open('{}/Model_{}_Performance.txt'.format(resultspath,foldnumber),'a') as fid:        fid.write('~~~~~~Fold {} Hyperparameters~~~~~~~'.format(foldnumber))        fid.write('\n')        fid.write('MAE: {:.3f}'.format(mae)+'\n')        fid.write('R2 : {:.3f}'.format(r2)+'\n')    trained_model.save('{}/model{}'.format(summariespath,foldnumber))    returnif __name__ == '__main__':    main()